{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af657043",
   "metadata": {},
   "source": [
    "# Pytorch的核心概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a299f",
   "metadata": {},
   "source": [
    "Pytorch是一个基于Python的机器学习库。它广泛应用于计算机视觉，自然语言处理等深度学习领域。是目前和TensorFlow分庭抗礼的深度学习框架，**在学术圈颇受欢迎**\n",
    "\n",
    "它主要提供了以下两种核心功能：\n",
    "\n",
    "1.支持GPU加速的张量计算。\n",
    "\n",
    "2.方便优化模型的自动微分机制。\n",
    "Pytorch的主要优点：\n",
    "\n",
    "1. 简洁易懂：Pytorch的API设计的相当简洁一致。基本上就是tensor, autograd, nn三级封装。\n",
    "\n",
    "2. 便于调试：Pytorch采用动态图，可以像普通Python代码一样进行调试。不同于TensorFlow, Pytorch的报错说明通常很容易看懂。\n",
    "\n",
    "3. 强大高效：Pytorch提供了非常丰富的模型组件，可以快速实现想法。并且运行速度很快。目前大部分深度学习相关的Paper都是用Pytorch实现的。\n",
    "\n",
    "Pytorch底层最核心的概念是:\n",
    "1. **张量**\n",
    "2. **自动微分**\n",
    "3. **动态计算图**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbbde4c",
   "metadata": {},
   "source": [
    "# 张量数据结构\n",
    "\n",
    "Pytorch的基本数据结构是张量Tensor。张量即多维数组\n",
    "\n",
    "Pytorch的张量和numpy中的array很类似\n",
    "本节内容：\n",
    "1. 张量的数据类型\n",
    "2. 张量的维度\n",
    "3. 张量的尺寸\n",
    "4. 张量和numpy数组"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d0485",
   "metadata": {},
   "source": [
    "## 张量的数据类型\n",
    "\n",
    "张量的数据类型和numpy.array基本一一对应，但是不支持str类型\n",
    "包括:\n",
    "1. torch.float64(torch.double),\n",
    "2. **torch.float32(torch.float)**\n",
    "3. torch.float16,\n",
    "4. torch.int64(torch.long),\n",
    "5. torch.int32(torch.int),\n",
    "6. torch.int16,\n",
    "7. torch.int8,\n",
    "8. torch.uint8,\n",
    "9. torch.bool\n",
    "\n",
    "**一般神经网络建模使用的都是torch.float32类型**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae923c",
   "metadata": {},
   "source": [
    "**自动判断数据类型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1faff61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) torch.int64\n",
      "tensor(2.) torch.float32\n",
      "tensor(True) torch.bool\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "i = torch.tensor(1);\n",
    "print(i, i.dtype)\n",
    "\n",
    "x = torch.tensor(2.0)\n",
    "print(x, x.dtype)\n",
    "\n",
    "b = torch.tensor(True)\n",
    "print(b, b.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd265c1",
   "metadata": {},
   "source": [
    "**指定数据类型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f30dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1, dtype=torch.int32) torch.int32\n",
      "tensor(2., dtype=torch.float64) torch.float64\n"
     ]
    }
   ],
   "source": [
    "i = torch.tensor(1, dtype= torch.int32)\n",
    "print(i, i.dtype)\n",
    "\n",
    "x = torch.tensor(2.0, dtype=torch.double)\n",
    "print(x, x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b341f8",
   "metadata": {},
   "source": [
    "**使用特定类构造函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba75e0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([712248608], dtype=torch.int32) torch.int32\n",
      "tensor(2.) torch.float32\n",
      "tensor(2.) torch.float32\n",
      "tensor([ True, False,  True, False]) torch.bool\n"
     ]
    }
   ],
   "source": [
    "i = torch.IntTensor(1)\n",
    "print(i, i.dtype)\n",
    "\n",
    "# 等价于Torch.FloatTensor\n",
    "x = torch.Tensor(np.array(2.0))\n",
    "print(x, x.dtype)\n",
    "\n",
    "#x_f = torch.FloatTensor(2.0)\n",
    "# 注意上面的写法会报错，data must be a sequence\n",
    "x_f = torch.FloatTensor(np.array(2.0))\n",
    "print(x_f, x_f.dtype)\n",
    "\n",
    "b = torch.BoolTensor(np.array([1,0,2,0]))\n",
    "print(b,b.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561be70",
   "metadata": {},
   "source": [
    "**不同类型进行转换**\n",
    "有三种方法：\n",
    "1. 调用float(), int()\n",
    "2. 调用type(torch.int) \n",
    "3. 调用type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98c8e39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) torch.int64\n",
      "tensor(1.) torch.float32\n",
      "tensor(1.) torch.float32\n",
      "tensor(1.) torch.float32\n"
     ]
    }
   ],
   "source": [
    "i = torch.tensor(1)\n",
    "print(i,i.dtype)\n",
    "\n",
    "#int64abs --> float32\n",
    "# 调用float()方法进行转换\n",
    "x = i.float()\n",
    "print(x, x.dtype)\n",
    "\n",
    "# 调用type()函数进行转换\n",
    "y = i.type(torch.float)\n",
    "print(y,y.dtype)\n",
    "\n",
    "#使用type_as转换成与指定张量类型相同的类型\n",
    "z = i.type_as(x)\n",
    "print(z,z.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c7369a",
   "metadata": {},
   "source": [
    "## 张量的维度\n",
    "\n",
    "不同类型的数据可以用不同维度(dimension)的张量来表示：\n",
    "1. 标量是0维张量\n",
    "2. 向量是1维张量\n",
    "3. 矩阵是2维张量\n",
    "\n",
    "...\n",
    "\n",
    "彩色图像有rgb三个通道，可以表示为3维张量\n",
    "\n",
    "视频还有时间维，可以表示为4维张量\n",
    "\n",
    "**可以简单地总结为：有几层中括号，就是多少维的张量**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39104d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标量\n",
      "tensor(True) 0\n",
      "向量\n",
      "tensor([1., 2., 3., 4., 5.]) 1\n",
      "矩阵\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]]) 2\n"
     ]
    }
   ],
   "source": [
    "# 标量\n",
    "scalar = torch.tensor(True)\n",
    "print(\"标量\")\n",
    "print(scalar, scalar.dim())\n",
    "\n",
    "# 向量 \n",
    "vector = torch.tensor([1.,2.,3.,4.,5.])\n",
    "print(\"向量\")\n",
    "print(vector, vector.dim())\n",
    "\n",
    "# 矩阵\n",
    "matrix = torch.tensor([[1.,2.,3.,],[4.,5.,6.]])\n",
    "print(\"矩阵\")\n",
    "print(matrix, matrix.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8beaf",
   "metadata": {},
   "source": [
    "## 张量的尺寸\n",
    "\n",
    "1. 可以使用` shape`属性或者 `size()`方法查看张量在每一维的长度\n",
    "2. 可以使用`view`方法改变张量的尺寸\n",
    "3. 如果`view`方法改变尺寸失败，可以使用`reshape`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7ceb7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(True)\n",
    "print(scalar.size())\n",
    "print(scalar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a46a47df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "vector = torch.tensor([1.,2.,3.,4.])\n",
    "print(vector.size())\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c223e765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.tensor([[1.,2.,3.],[4.,5.,6.]])\n",
    "print(matrix.size())\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4e9d9",
   "metadata": {},
   "source": [
    "**使用view改变张量尺寸**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "307c3356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "vector = torch.arange(0,12)\n",
    "# vector  = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
    "print(vector.shape)\n",
    "\n",
    "matrix34 = vector.view(3,4)\n",
    "print(matrix34.shape)\n",
    "\n",
    "# -1 表示该位置长度由程序自动计算\n",
    "matrix43 = vector.view(4, -1)\n",
    "print(matrix43.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67cc01e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6])\n",
      "False\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 有些操作会让张量存储结构扭曲，直接使用view会失败，可以用reshape方法\n",
    "matrix26 = torch.arange(0,12).view(2,6)\n",
    "print(matrix26.shape)\n",
    "\n",
    "# 转置操作让张量存储结构不连续\n",
    "# Tensor.is_contiguous(memory_format=torch.contiguous_format) → boolabs\n",
    "#Returns True if self tensor is contiguous in memory in the order specified by memory format.\n",
    "matrix62 = matrix26.t()\n",
    "print(matrix62.is_contiguous())\n",
    "\n",
    "# 直接使用view方法会失败，可以使用reshape方法\n",
    "#matrix34 = matrix62.view(3,4) #error!\n",
    "matrix34 = matrix62.reshape(3,4)\n",
    "# 等价于matrix34 = matrix62.contiguous().view(3,4)\n",
    "print(matrix34.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1096c48",
   "metadata": {},
   "source": [
    "## 张量和numpy数组\n",
    "\n",
    "- numpy() : 从Tensor中得到numpy数组\n",
    "- torch.from_numpy(): 从numpy中得到Tensor\n",
    "\n",
    "注意：**这两种方法修改的对象是同一个内存地址，如果改变其中一个，另外一个的值也会发生改变**。如果不想共享数据内存，可以用使用clone()方法克隆张量\n",
    "\n",
    "- item(): 从Tensor中得到对应的标量值，即Tensor --> scalar\n",
    "\n",
    "- tolist() :从张量得到对应的Python数值列表，即scalar-->Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e4d8308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在添加1之前：\n",
      "[0. 0. 0.]\n",
      "tensor([0., 0., 0.], dtype=torch.float64)\n",
      "在添加1之后:\n",
      "[1. 1. 1.]\n",
      "tensor([1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# numpy --Tensor\n",
    "arr = np.zeros(3)\n",
    "tensor = torch.from_numpy(arr)\n",
    "print(\"在添加1之前：\")\n",
    "print(arr)\n",
    "print(tensor)\n",
    "\n",
    "print(\"在添加1之后:\")\n",
    "# 给numpy类型的arr对象加1，tensor也会随之改变\n",
    "np.add(arr, 1, out=arr)\n",
    "print(arr)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0fc7aea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在添加1之前：\n",
      "tensor([0., 0., 0.])\n",
      "[0. 0. 0.]\n",
      "在添加1之后：\n",
      "tensor([1., 1., 1.])\n",
      "[1. 1. 1.]\n",
      "使用add_()执行加1 操作:\n",
      "tensor([2., 2., 2.])\n",
      "[2. 2. 2.]\n",
      "使用add()方法执行加一操作：\n",
      "mytensor= tensor([2., 2., 2.])\n",
      "new_tensor= tensor([3., 3., 3.])\n",
      "arr =  [2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Tensor -->numpy\n",
    "mytensor = torch.zeros(3)\n",
    "arr = mytensor.numpy()\n",
    "print(\"在添加1之前：\")\n",
    "print(mytensor)\n",
    "print(arr)\n",
    "\n",
    "print(\"在添加1之后：\")\n",
    "torch.add(mytensor,1, out=mytensor)\n",
    "print(mytensor)\n",
    "print(arr)\n",
    "# 或者：加1操作可以使用add_()原地操作方法\n",
    "print(\"使用add_()执行加1 操作:\")\n",
    "mytensor.add_(1)\n",
    "print(mytensor)\n",
    "print(arr)\n",
    "# 注意使用add()不是改变原数组\n",
    "print(\"使用add()方法执行加一操作：\")\n",
    "new_tensor = mytensor.add(1)\n",
    "print(\"mytensor=\",mytensor)\n",
    "print(\"new_tensor=\",new_tensor)\n",
    "print(\"arr = \",arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a44e842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在执行加1操作之前：\n",
      "tensor([0., 0., 0.])\n",
      "[0. 0. 0.]\n",
      "在执行加1操作之后：\n",
      "tensor([1., 1., 1.])\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 使用 clone()方法拷贝张量\n",
    "mytensor = torch.zeros(3)\n",
    "arr = mytensor.clone().numpy()\n",
    "print(\"在执行加1操作之前：\")\n",
    "print(mytensor)\n",
    "print(arr)\n",
    "\n",
    "print(\"在执行加1操作之后：\")\n",
    "mytensor.add_(1)\n",
    "print(mytensor)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb5463c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "1.0\n",
      "<class 'float'>\n",
      "tensor([[0.6307, 0.8646],\n",
      "        [0.4512, 0.9578]])\n",
      "[[0.6306673288345337, 0.8646149635314941], [0.45118987560272217, 0.957770049571991]]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(1.0)\n",
    "s = scalar.item()\n",
    "print(scalar)\n",
    "print(s)\n",
    "print(type(s))\n",
    "\n",
    "mytensor = torch.rand(2,2)\n",
    "t = mytensor.tolist()\n",
    "print(mytensor)\n",
    "print(t)\n",
    "print(type(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1639cc60",
   "metadata": {},
   "source": [
    "# 自动微分机制\n",
    "\n",
    "神经网络通常依赖`反向传播`**求梯度**来更新网络参数\n",
    "\n",
    "Pytorch一般通过反向传播 backward 方法 实现这种求梯度计算,该方法求得的梯度将存在对应自变量张量的grad属性下\n",
    "\n",
    "除此之外，也能够调用torch.autograd.grad 函数来实现求梯度计算。\n",
    "\n",
    "这就是Pytorch的自动微分机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3578f94",
   "metadata": {},
   "source": [
    "## 利用backward方法求导数\n",
    "\n",
    "backward 方法通常在一个**标量**张量上调用，该方法求得的梯度将存在对应自变量张量的grad属性下\n",
    "\n",
    "如果调用的张量非标量，则要传入一个和它同形状的gradient参数张量。\n",
    "\n",
    "相当于用该gradient参数张量与调用张量作向量点乘，得到的标量结果再反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f91d6f",
   "metadata": {},
   "source": [
    "一、**标量的反向传播**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d6a0787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "import numpy as py\n",
    "import torch\n",
    "\n",
    "# f(x) = a*x^2 + b*x + c \n",
    "\n",
    "# requires_grad = True 表示需要被求导\n",
    "x = torch.tensor(0.0, requires_grad = True)\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "\n",
    "y = a*torch.pow(x,2) + b*x + c\n",
    "\n",
    "y.backward()\n",
    "# 求x的梯度\n",
    "dy_dx = x.grad\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb749ee6",
   "metadata": {},
   "source": [
    "**二、非标量的反向传播**\n",
    "\n",
    "如果是二次使用反向传播机制，y.backward()， 则需要添加`retain_graph=True`参数，否则会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca53206e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_grad_new =  tensor([[-4., -8.],\n",
      "        [ 0., 10.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "# f(x) = a*x^2 + b*x + c \n",
    "x = torch.tensor([[0.,0.],[1.,2.]], requires_grad = True)\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "\n",
    "y = a*torch.pow(x,2) + b*x + c\n",
    "\n",
    "# gradient_tensor = torch.tensor([[1.,1.],[1.,1.]])\n",
    "# y.backward(gradient=gradient_tensor)\n",
    "# \n",
    "#Q: 是否和传入的tensor有关系呢？\n",
    "#A: 和传入的tensor有关系，如果是传入的tensor是全0， 则反向传播的结果全是零\n",
    "# result = grad_tensor * (dy / dx)\n",
    "# gradient_tensor_zero = torch.tensor([[0.,0.],[0.,0.]])\n",
    "# y.backward(gradient=gradient_tensor_zero)\n",
    "gradient_tensor_non_zero = torch.tensor([[2.,4.],[3.,5.]])\n",
    "y.backward(gradient = gradient_tensor_non_zero)\n",
    "\n",
    "x_grad = x.grad\n",
    "print(\"x_grad_new = \", x_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf01914d",
   "metadata": {},
   "source": [
    "**三、非标量的反向传播的另一个实现**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b040aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2., -2.],\n",
      "        [ 0.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "# f(x) = a*x^2 + b*x + c \n",
    "x = torch.tensor([[0.,0.],[1.,2.]], requires_grad = True)\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "\n",
    "y = a*torch.pow(x,2) + b*x + c\n",
    "\n",
    "gradient_tensor = torch.tensor([[1.,1.],[1.,1.]])\n",
    "# z是标量，是求和得到的\n",
    "z = torch.sum(y*gradient_tensor)\n",
    "\n",
    "z.backward()\n",
    "x_grad = x.grad\n",
    "print(x_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace8556",
   "metadata": {},
   "source": [
    "## 利用autograd.grad方法求导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "954c2362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# f(x) = a*x^2 + b*x + c \n",
    "x = torch.tensor(0.0, requires_grad = True)\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "\n",
    "y = a*torch.pow(x,2) + b*x + c\n",
    "\n",
    "# create_graph 设置为True将允许创建更高阶的导数,如求二阶导数\n",
    "dy_dx = torch.autograd.grad(y,x, create_graph=True)[0]\n",
    "print(dy_dx.data)\n",
    "\n",
    "# 求二阶倒数\n",
    "dy2_dx2 = torch.autograd.grad(dy_dx, x)[0]\n",
    "print(dy2_dx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "144af329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.) tensor(1.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(3.) tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "x1 = torch.tensor(1.0, requires_grad = True)\n",
    "x2 = torch.tensor(2.0, requires_grad = True)\n",
    "\n",
    "y1 = x1*x2\n",
    "y2 = x1+x2\n",
    "\n",
    "# 允许同时对多个自动变量求导数\n",
    "(dy1_dx1, dy1_dx2) = torch.autograd.grad(outputs=y1, inputs=[x1,x2], retain_graph=True)\n",
    "print(dy1_dx1, dy1_dx2)\n",
    "(dy2_dx1, dy2_dx2) = torch.autograd.grad(outputs=y2, inputs=[x1,x2], retain_graph=True)\n",
    "print(dy2_dx1, dy2_dx2)\n",
    "\n",
    "# 如果有多个因变量，相当于把多个因变量的梯度结果求和\n",
    "(dy12_dx1, dy12_dx2) = torch.autograd.grad(outputs=[y1, y2],inputs=[x1, x2])\n",
    "print(dy12_dx1, dy12_dx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfbd8fd",
   "metadata": {},
   "source": [
    "## 利用自动微分和优化器求最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb9b410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y =  tensor(0.)\n",
      "x =  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "# f(x) = a*x^2 + b*x + c \n",
    "x = torch.tensor(0.0, requires_grad = True)\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.SGD(params=[x], lr = 0.01)\n",
    "\n",
    "def f(x):\n",
    "    result = a*torch.pow(x,2)+b*x+c\n",
    "    return(result)\n",
    "\n",
    "for i in range(500):\n",
    "    # 梯度清零\n",
    "    optimizer.zero_grad()\n",
    "    y = f(x)\n",
    "    # 反向传播求梯度\n",
    "    y.backward()\n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "    \n",
    "print(\"y = \", f(x).data)\n",
    "print(\"x = \", x.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf16e5",
   "metadata": {},
   "source": [
    "# 动态计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088321fe",
   "metadata": {},
   "source": [
    "    本节Pytorch的动态计算图：\n",
    "    \n",
    "    1. 动态计算图简介\n",
    "    2. 计算图中的Function\n",
    "    3. 计算图和反向传播\n",
    "    4. 叶子节点和非叶子节点\n",
    "    5. 计算图在TensorBoard中的可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de371f",
   "metadata": {},
   "source": [
    "## 动态计算图简介\n",
    "\n",
    "计算图是用来描述运算的有向无环图，计算图有两个主要元素：**结点（Node）和边（Edge）**\n",
    "\n",
    "说法一：Pytorch的计算图由节点和边组成，节点表示张量或者Function，边表示张量和Function之间的依赖关系\n",
    "\n",
    "说法二：\n",
    "- 结点表示数据，如向量，矩阵，张量, function \n",
    "- 边表示运算，如加减乘除卷积等\n",
    "\n",
    "我的想法：节点表示的是数据和函数，边表示的是依赖关系\n",
    "Pytorch中的计算图是动态图。这里的动态主要有两重含义:\n",
    "\n",
    "1. **计算图的正向传播是立即执行的**。无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到计算结果。\n",
    "2. **计算图在反向传播后立即销毁**。下次调用需要重新构建计算图。如果在程序中使用了backward方法执行了反向传播，或者利用torch.autograd.grad方法计算了梯度，那么创建的计算图会被立即销毁，释放存储空间，下次调用需要重新创建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc774a5",
   "metadata": {},
   "source": [
    "**一、计算图的正向传播是立即执行的**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98338a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.6364)\n",
      "tensor([[ 2.9136],\n",
      "        [ 5.3460],\n",
      "        [ 8.8638],\n",
      "        [ 6.9459],\n",
      "        [ 2.8933],\n",
      "        [ 6.8978],\n",
      "        [-4.3608],\n",
      "        [ 7.4888],\n",
      "        [ 0.7668],\n",
      "        [ 5.7776]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor([[3.0,1.0]],requires_grad = True)\n",
    "b = torch.tensor([[3.0]], requires_grad = True)\n",
    "\n",
    "# 生成一个10*2的矩阵 \n",
    "X = torch.randn(10,2)\n",
    "# 生成一个10*1的矩阵\n",
    "Y = torch.randn(10,1)\n",
    "# Y_hat定义后其正向传播被立即执行，与后面的loss创建语句无关\n",
    "\n",
    "# @是Python 3.5之后加入的矩阵乘法运算符\n",
    "Y_hat = X@w.t() + b \n",
    "# 均方差\n",
    "loss = torch.mean(torch.pow(Y_hat-Y,2))\n",
    "\n",
    "print(loss.data)\n",
    "print(Y_hat.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e335d46",
   "metadata": {},
   "source": [
    "**二、计算图在反向传播后立即销毁**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adc7a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor([[3.0,1.0]],requires_grad = True)\n",
    "b = torch.tensor([[3.0]], requires_grad = True)\n",
    "\n",
    "# 生成一个10*2的矩阵 \n",
    "X = torch.randn(10,2)\n",
    "# 生成一个10*1的矩阵\n",
    "Y = torch.randn(10,1)\n",
    "# Y_hat定义后其正向传播被立即执行，与后面的loss创建语句无关\n",
    "\n",
    "# @是Python 3.5之后加入的矩阵乘法运算符\n",
    "Y_hat = X@w.t() + b \n",
    "# 均方差\n",
    "loss = torch.mean(torch.pow(Y_hat-Y,2))\n",
    "\n",
    "# 计算图在反向传播完成后立即被销毁，如果需要保留计算图，则需要设置retain_graph = True 的参数\n",
    "# loss.backward(retain_graph=True)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49fa283",
   "metadata": {},
   "source": [
    "## 计算图中的Function \n",
    "\n",
    "计算图中的 张量我们已经比较熟悉了, 计算图中的另外一种节点是Function, 实际上就是 Pytorch中各种对张量操作的函数。\n",
    "\n",
    "这些Function和我们Python中的函数有一个较大的区别，那就是它同时包括正向计算逻辑和反向传播的逻辑\n",
    "\n",
    "**我们可以通过继承torch.autograd.Function来创建这种支持反向传播的Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e74c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    #正向传播逻辑, 使用ctx存储一些值，供反向传播使用\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    # 反向传播逻辑\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] =0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9781a7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000]])\n",
      "tensor([[4.5000]])\n",
      "<torch.autograd.function.MyReLUBackward object at 0x7fb21593fcf8>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor([[3.0,1.0]], requires_grad = True)\n",
    "b = torch.tensor([[3.0]], requires_grad=True)\n",
    "\n",
    "X = torch.tensor([[-1.0,-1.0],[1.0,1.0]])\n",
    "Y = torch.tensor([[2.0,3.0]])\n",
    "\n",
    "relu = MyReLU.apply\n",
    "\n",
    "Y_hat = relu(X@w.t() + b)\n",
    "loss = torch.mean(torch.pow(Y_hat-Y, 2))\n",
    "\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "\n",
    "# Y_hat的梯度函数即是我们自己所定义的 MyReLU.backward\n",
    "print(Y_hat.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc5ec78",
   "metadata": {},
   "source": [
    "## 计算图与反向传播\n",
    "\n",
    "了解了Function的功能，我们可以简单地理解一下反向传播的原理和过程。理解该部分原理需要一些高等数学中求导链式法则的基础知识\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0caa820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.tensor(3.0,requires_grad=True)\n",
    "y1 = x + 1\n",
    "y2 = 2*x\n",
    "loss = (y1-y2)**2\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b067a90",
   "metadata": {},
   "source": [
    "loss.backward()语句调用后，依次发生以下计算过程。\n",
    "\n",
    "1. loss自己的grad梯度赋值为1，即对自身的梯度为1\n",
    "2. loss根据其自身梯度以及关联的backward方法，计算出其对应的自变量即y1和y2的梯度，将该值赋值到y1.grad和y2.grad\n",
    "3. y2和y1根据其自身梯度以及关联的backward方法, 分别计算出其对应的自变量x的梯度，x.grad将其收到的多个梯度值累加\n",
    "\n",
    "**正因为求导链式法则衍生的梯度累加规则，张量的grad梯度不会自动清零，在需要的时候需要手动置零**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a0a5f8",
   "metadata": {},
   "source": [
    "## 叶子节点和非叶子节点\n",
    "\n",
    "执行下面代码，我们会发现 loss.grad并不是我们期望的1,而是 None。类似地 y1.grad 以及 y2.grad也是 None.\n",
    "\n",
    "Why?\n",
    "\n",
    "这是由于它们不是叶子节点张量\n",
    "\n",
    "在反向传播过程中，只有 is_leaf=True 的叶子节点，需要求导的张量的导数结果才会被最后保留下来.\n",
    "\n",
    "叶子节点张量需要满足两个条件:\n",
    "1. 叶子节点张量是由用户直接创建的张量，而非由某个Function通过计算得到的张量\n",
    "2. 叶子节点张量的 requires_grad属性必须为True\n",
    "\n",
    "Pytorch设计这样的规则主要是为了节约内存或者显存空间，因为几乎所有的时候，用户只会关心他自己直接创建的张量的梯度。\n",
    "所有非叶子节点的张量, 其requires_grad 属性必定是True的，但其梯度值只在计算过程中被用到，不会最终存储到grad属性中。如果需要保留中间计算结果的梯度到grad属性中，可以使用 retain_grad方法。 如果仅仅是为了调试代码查看梯度值，可以利用register_hook打印日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "540c5096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.grad: None\n",
      "y1.grad: None\n",
      "y2.grad: None\n",
      "tensor(4.)\n",
      "====================\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beta/anaconda3/envs/lye/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  if __name__ == '__main__':\n",
      "/home/beta/anaconda3/envs/lye/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/beta/anaconda3/envs/lye/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.tensor(3.0,requires_grad=True)\n",
    "y1 = x + 1\n",
    "y2 = 2*x\n",
    "loss = (y1-y2)**2\n",
    "\n",
    "loss.backward()\n",
    "print(\"loss.grad:\", loss.grad)\n",
    "print(\"y1.grad:\", y1.grad)\n",
    "print(\"y2.grad:\", y2.grad)\n",
    "print(x.grad)\n",
    "print(\"=\"*20)\n",
    "print(x.is_leaf)\n",
    "print(y1.is_leaf)\n",
    "print(y2.is_leaf)\n",
    "print(loss.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809895f9",
   "metadata": {},
   "source": [
    "> - 利用retain_grad可以保留非叶子节点的梯度值\n",
    "> - 利用register_hook可以查看非叶子节点的梯度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6674a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y2 grad: tensor(4.)\n",
      "y1 grad: tensor(-4.)\n",
      "loss grad: tensor(1.)\n",
      "x.grad tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 正向传播\n",
    "x = torch.tensor(3.0, requires_grad = True)\n",
    "y1 = x + 1\n",
    "y2 = 2*x\n",
    "loss = (y1- y2) **2\n",
    "\n",
    "# 非叶子节点梯度显示控制\n",
    "y1.register_hook(lambda grad: print('y1 grad:', grad))\n",
    "y2.register_hook(lambda grad: print('y2 grad:', grad))\n",
    "\n",
    "loss.retain_grad()\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "print('loss grad:', loss.grad)\n",
    "print('x.grad', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03e035",
   "metadata": {},
   "source": [
    "## 计算图在TensorBoard中的可视化\n",
    "\n",
    "可以利用 torch.utils.tensorboard 将计算图导出到 TensorBoard进行可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c986d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.w = nn.Parameter(torch.randn(2,1))\n",
    "        self.b = nn.Parameter(torch.randn(1,1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = x@self.w +self.b\n",
    "        return y\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1445b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('./data/tensorboard')\n",
    "writer.add_graph(net, input_to_model=torch.rand(10,2))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5472c27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Known TensorBoard instances:\n",
      "  - port 6007: logdir ./data/tensorboard (started 0:07:30 ago; pid 12992)\n",
      "  - port 6006: logdir ./data/tensorboard (started 0:10:43 ago; pid 12885)\n",
      "  - port 6006: logdir ./data/tensorboard (started 0:07:22 ago; pid 13098)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13098), started 0:07:22 ago. (Use '!kill 13098' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3ba5603422e70c77\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3ba5603422e70c77\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "#在tensorboard中查看模型\n",
    "from tensorboard import notebook\n",
    "notebook.list() \n",
    "notebook.start(\"--logdir ./data/tensorboard --host=10.20.13.18\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
