{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af657043",
   "metadata": {},
   "source": [
    "# Pytorch的核心概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a299f",
   "metadata": {},
   "source": [
    "Pytorch是一个基于Python的机器学习库。它广泛应用于计算机视觉，自然语言处理等深度学习领域。是目前和TensorFlow分庭抗礼的深度学习框架，**在学术圈颇受欢迎**\n",
    "\n",
    "它主要提供了以下两种核心功能：\n",
    "\n",
    "1.支持GPU加速的张量计算。\n",
    "\n",
    "2.方便优化模型的自动微分机制。\n",
    "Pytorch的主要优点：\n",
    "\n",
    "1. 简洁易懂：Pytorch的API设计的相当简洁一致。基本上就是tensor, autograd, nn三级封装。\n",
    "\n",
    "2. 便于调试：Pytorch采用动态图，可以像普通Python代码一样进行调试。不同于TensorFlow, Pytorch的报错说明通常很容易看懂。\n",
    "\n",
    "3. 强大高效：Pytorch提供了非常丰富的模型组件，可以快速实现想法。并且运行速度很快。目前大部分深度学习相关的Paper都是用Pytorch实现的。\n",
    "\n",
    "Pytorch底层最核心的概念是:\n",
    "1. **张量**\n",
    "2. **自动微分**\n",
    "3. **动态计算图**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbbde4c",
   "metadata": {},
   "source": [
    "# 张量数据结构\n",
    "\n",
    "Pytorch的基本数据结构是张量Tensor。张量即多维数组\n",
    "\n",
    "Pytorch的张量和numpy中的array很类似\n",
    "本节内容：\n",
    "1. 张量的数据类型\n",
    "2. 张量的维度\n",
    "3. 张量的尺寸\n",
    "4. 张量和numpy数组"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d0485",
   "metadata": {},
   "source": [
    "## 张量的数据类型\n",
    "\n",
    "张量的数据类型和numpy.array基本一一对应，但是不支持str类型\n",
    "包括:\n",
    "1. torch.float64(torch.double),\n",
    "2. **torch.float32(torch.float)**\n",
    "3. torch.float16,\n",
    "4. torch.int64(torch.long),\n",
    "5. torch.int32(torch.int),\n",
    "6. torch.int16,\n",
    "7. torch.int8,\n",
    "8. torch.uint8,\n",
    "9. torch.bool\n",
    "\n",
    "**一般神经网络建模使用的都是torch.float32类型**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae923c",
   "metadata": {},
   "source": [
    "**自动判断数据类型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1faff61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) torch.int64\n",
      "tensor(2.) torch.float32\n",
      "tensor(True) torch.bool\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "i = torch.tensor(1);\n",
    "print(i, i.dtype)\n",
    "\n",
    "x = torch.tensor(2.0)\n",
    "print(x, x.dtype)\n",
    "\n",
    "b = torch.tensor(True)\n",
    "print(b, b.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd265c1",
   "metadata": {},
   "source": [
    "**指定数据类型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f30dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1, dtype=torch.int32) torch.int32\n",
      "tensor(2., dtype=torch.float64) torch.float64\n"
     ]
    }
   ],
   "source": [
    "i = torch.tensor(1, dtype= torch.int32)\n",
    "print(i, i.dtype)\n",
    "\n",
    "x = torch.tensor(2.0, dtype=torch.double)\n",
    "print(x, x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b341f8",
   "metadata": {},
   "source": [
    "**使用特定类构造函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba75e0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([712248608], dtype=torch.int32) torch.int32\n",
      "tensor(2.) torch.float32\n",
      "tensor(2.) torch.float32\n",
      "tensor([ True, False,  True, False]) torch.bool\n"
     ]
    }
   ],
   "source": [
    "i = torch.IntTensor(1)\n",
    "print(i, i.dtype)\n",
    "\n",
    "# 等价于Torch.FloatTensor\n",
    "x = torch.Tensor(np.array(2.0))\n",
    "print(x, x.dtype)\n",
    "\n",
    "#x_f = torch.FloatTensor(2.0)\n",
    "# 注意上面的写法会报错，data must be a sequence\n",
    "x_f = torch.FloatTensor(np.array(2.0))\n",
    "print(x_f, x_f.dtype)\n",
    "\n",
    "b = torch.BoolTensor(np.array([1,0,2,0]))\n",
    "print(b,b.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561be70",
   "metadata": {},
   "source": [
    "**不同类型进行转换**\n",
    "有三种方法：\n",
    "1. 调用float(), int()\n",
    "2. 调用type(torch.int) \n",
    "3. 调用type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98c8e39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) torch.int64\n",
      "tensor(1.) torch.float32\n",
      "tensor(1.) torch.float32\n",
      "tensor(1.) torch.float32\n"
     ]
    }
   ],
   "source": [
    "i = torch.tensor(1)\n",
    "print(i,i.dtype)\n",
    "\n",
    "#int64abs --> float32\n",
    "# 调用float()方法进行转换\n",
    "x = i.float()\n",
    "print(x, x.dtype)\n",
    "\n",
    "# 调用type()函数进行转换\n",
    "y = i.type(torch.float)\n",
    "print(y,y.dtype)\n",
    "\n",
    "#使用type_as转换成与指定张量类型相同的类型\n",
    "z = i.type_as(x)\n",
    "print(z,z.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c7369a",
   "metadata": {},
   "source": [
    "## 张量的维度\n",
    "\n",
    "不同类型的数据可以用不同维度(dimension)的张量来表示：\n",
    "1. 标量是0维张量\n",
    "2. 向量是1维张量\n",
    "3. 矩阵是2维张量\n",
    "\n",
    "...\n",
    "\n",
    "彩色图像有rgb三个通道，可以表示为3维张量\n",
    "\n",
    "视频还有时间维，可以表示为4维张量\n",
    "\n",
    "**可以简单地总结为：有几层中括号，就是多少维的张量**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39104d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标量\n",
      "tensor(True) 0\n",
      "向量\n",
      "tensor([1., 2., 3., 4., 5.]) 1\n",
      "矩阵\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]]) 2\n"
     ]
    }
   ],
   "source": [
    "# 标量\n",
    "scalar = torch.tensor(True)\n",
    "print(\"标量\")\n",
    "print(scalar, scalar.dim())\n",
    "\n",
    "# 向量 \n",
    "vector = torch.tensor([1.,2.,3.,4.,5.])\n",
    "print(\"向量\")\n",
    "print(vector, vector.dim())\n",
    "\n",
    "# 矩阵\n",
    "matrix = torch.tensor([[1.,2.,3.,],[4.,5.,6.]])\n",
    "print(\"矩阵\")\n",
    "print(matrix, matrix.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8beaf",
   "metadata": {},
   "source": [
    "## 张量的尺寸\n",
    "\n",
    "1. 可以使用` shape`属性或者 `size()`方法查看张量在每一维的长度\n",
    "2. 可以使用`view`方法改变张量的尺寸\n",
    "3. 如果`view`方法改变尺寸失败，可以使用`reshape`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7ceb7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(True)\n",
    "print(scalar.size())\n",
    "print(scalar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a46a47df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "vector = torch.tensor([1.,2.,3.,4.])\n",
    "print(vector.size())\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c223e765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.tensor([[1.,2.,3.],[4.,5.,6.]])\n",
    "print(matrix.size())\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d4e9d9",
   "metadata": {},
   "source": [
    "**使用view改变张量尺寸**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "307c3356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "vector = torch.arange(0,12)\n",
    "# vector  = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
    "print(vector.shape)\n",
    "\n",
    "matrix34 = vector.view(3,4)\n",
    "print(matrix34.shape)\n",
    "\n",
    "# -1 表示该位置长度由程序自动计算\n",
    "matrix43 = vector.view(4, -1)\n",
    "print(matrix43.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67cc01e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6])\n",
      "False\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 有些操作会让张量存储结构扭曲，直接使用view会失败，可以用reshape方法\n",
    "matrix26 = torch.arange(0,12).view(2,6)\n",
    "print(matrix26.shape)\n",
    "\n",
    "# 转置操作让张量存储结构不连续\n",
    "# Tensor.is_contiguous(memory_format=torch.contiguous_format) → boolabs\n",
    "#Returns True if self tensor is contiguous in memory in the order specified by memory format.\n",
    "matrix62 = matrix26.t()\n",
    "print(matrix62.is_contiguous())\n",
    "\n",
    "# 直接使用view方法会失败，可以使用reshape方法\n",
    "#matrix34 = matrix62.view(3,4) #error!\n",
    "matrix34 = matrix62.reshape(3,4)\n",
    "# 等价于matrix34 = matrix62.contiguous().view(3,4)\n",
    "print(matrix34.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1096c48",
   "metadata": {},
   "source": [
    "## 张量和numpy数组\n",
    "\n",
    "- numpy() : 从Tensor中得到numpy数组\n",
    "- torch.from_numpy(): 从numpy中得到Tensor\n",
    "\n",
    "注意：**这两种方法修改的对象是同一个内存地址，如果改变其中一个，另外一个的值也会发生改变**。如果不想共享数据内存，可以用使用clone()方法克隆张量\n",
    "\n",
    "- item(): 从Tensor中得到对应的标量值，即Tensor --> scalar\n",
    "\n",
    "- tolist() :从张量得到对应的Python数值列表，即scalar-->Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e4d8308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在添加1之前：\n",
      "[0. 0. 0.]\n",
      "tensor([0., 0., 0.], dtype=torch.float64)\n",
      "在添加1之后:\n",
      "[1. 1. 1.]\n",
      "tensor([1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# numpy --Tensor\n",
    "arr = np.zeros(3)\n",
    "tensor = torch.from_numpy(arr)\n",
    "print(\"在添加1之前：\")\n",
    "print(arr)\n",
    "print(tensor)\n",
    "\n",
    "print(\"在添加1之后:\")\n",
    "# 给numpy类型的arr对象加1，tensor也会随之改变\n",
    "np.add(arr, 1, out=arr)\n",
    "print(arr)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0fc7aea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在添加1之前：\n",
      "tensor([0., 0., 0.])\n",
      "[0. 0. 0.]\n",
      "在添加1之后：\n",
      "tensor([1., 1., 1.])\n",
      "[1. 1. 1.]\n",
      "使用add_()执行加1 操作:\n",
      "tensor([2., 2., 2.])\n",
      "[2. 2. 2.]\n",
      "使用add()方法执行加一操作：\n",
      "mytensor= tensor([2., 2., 2.])\n",
      "new_tensor= tensor([3., 3., 3.])\n",
      "arr =  [2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Tensor -->numpy\n",
    "mytensor = torch.zeros(3)\n",
    "arr = mytensor.numpy()\n",
    "print(\"在添加1之前：\")\n",
    "print(mytensor)\n",
    "print(arr)\n",
    "\n",
    "print(\"在添加1之后：\")\n",
    "torch.add(mytensor,1, out=mytensor)\n",
    "print(mytensor)\n",
    "print(arr)\n",
    "# 或者：加1操作可以使用add_()原地操作方法\n",
    "print(\"使用add_()执行加1 操作:\")\n",
    "mytensor.add_(1)\n",
    "print(mytensor)\n",
    "print(arr)\n",
    "# 注意使用add()不是改变原数组\n",
    "print(\"使用add()方法执行加一操作：\")\n",
    "new_tensor = mytensor.add(1)\n",
    "print(\"mytensor=\",mytensor)\n",
    "print(\"new_tensor=\",new_tensor)\n",
    "print(\"arr = \",arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a44e842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在执行加1操作之前：\n",
      "tensor([0., 0., 0.])\n",
      "[0. 0. 0.]\n",
      "在执行加1操作之后：\n",
      "tensor([1., 1., 1.])\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 使用 clone()方法拷贝张量\n",
    "mytensor = torch.zeros(3)\n",
    "arr = mytensor.clone().numpy()\n",
    "print(\"在执行加1操作之前：\")\n",
    "print(mytensor)\n",
    "print(arr)\n",
    "\n",
    "print(\"在执行加1操作之后：\")\n",
    "mytensor.add_(1)\n",
    "print(mytensor)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb5463c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "1.0\n",
      "<class 'float'>\n",
      "tensor([[0.6307, 0.8646],\n",
      "        [0.4512, 0.9578]])\n",
      "[[0.6306673288345337, 0.8646149635314941], [0.45118987560272217, 0.957770049571991]]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor(1.0)\n",
    "s = scalar.item()\n",
    "print(scalar)\n",
    "print(s)\n",
    "print(type(s))\n",
    "\n",
    "mytensor = torch.rand(2,2)\n",
    "t = mytensor.tolist()\n",
    "print(mytensor)\n",
    "print(t)\n",
    "print(type(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1639cc60",
   "metadata": {},
   "source": [
    "# 自动微分机制\n",
    "\n",
    "神经网络通常依赖`反向传播`**求梯度**来更新网络参数\n",
    "\n",
    "Pytorch一般通过反向传播 backward 方法 实现这种求梯度计算,该方法求得的梯度将存在对应自变量张量的grad属性下\n",
    "\n",
    "除此之外，也能够调用torch.autograd.grad 函数来实现求梯度计算。\n",
    "\n",
    "这就是Pytorch的自动微分机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3578f94",
   "metadata": {},
   "source": [
    "## 利用backward方法求导数\n",
    "\n",
    "backward 方法通常在一个**标量**张量上调用，该方法求得的梯度将存在对应自变量张量的grad属性下\n",
    "\n",
    "如果调用的张量非标量，则要传入一个和它同形状的gradient参数张量。\n",
    "\n",
    "相当于用该gradient参数张量与调用张量作向量点乘，得到的标量结果再反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f91d6f",
   "metadata": {},
   "source": [
    "一、**标量的反向传播**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d6a0787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "import numpy as py\n",
    "import torch\n",
    "\n",
    "# f(x) = a*x^2 + b*x + c \n",
    "\n",
    "# requires_grad = True 表示需要被求导\n",
    "x = torch.tensor(0.0, requires_grad = True)\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "\n",
    "y = a*torch.pow(x,2) + b*x + c\n",
    "\n",
    "y.backward()\n",
    "# 求x的梯度\n",
    "dy_dx = x.grad\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4658b2fc",
   "metadata": {},
   "source": [
    "**二、非标量的反向传播**\n",
    "\n",
    "如果是二次使用反向传播机制，y.backward()， 则需要添加`retain_graph=True`参数，否则会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d656c6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_grad_new =  tensor([[-4., -8.],\n",
      "        [ 0., 10.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "# f(x) = a*x^2 + b*x + c \n",
    "x = torch.tensor([[0.,0.],[1.,2.]], requires_grad = True)\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "\n",
    "y = a*torch.pow(x,2) + b*x + c\n",
    "\n",
    "# gradient_tensor = torch.tensor([[1.,1.],[1.,1.]])\n",
    "# y.backward(gradient=gradient_tensor)\n",
    "# \n",
    "#Q: 是否和传入的tensor有关系呢？\n",
    "#A: 和传入的tensor有关系，如果是传入的tensor是全0， 则反向传播的结果全是零\n",
    "# result = grad_tensor * (dy / dx)\n",
    "# gradient_tensor_zero = torch.tensor([[0.,0.],[0.,0.]])\n",
    "# y.backward(gradient=gradient_tensor_zero)\n",
    "gradient_tensor_non_zero = torch.tensor([[2.,4.],[3.,5.]])\n",
    "y.backward(gradient = gradient_tensor_non_zero)\n",
    "\n",
    "x_grad = x.grad\n",
    "print(\"x_grad_new = \", x_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b3856",
   "metadata": {},
   "source": [
    "**三、非标量的反向传播的另一个实现**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38f8d262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2., -2.],\n",
      "        [ 0.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "# f(x) = a*x^2 + b*x + c \n",
    "x = torch.tensor([[0.,0.],[1.,2.]], requires_grad = True)\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "\n",
    "y = a*torch.pow(x,2) + b*x + c\n",
    "\n",
    "gradient_tensor = torch.tensor([[1.,1.],[1.,1.]])\n",
    "# z是标量，是求和得到的\n",
    "z = torch.sum(y*gradient_tensor)\n",
    "\n",
    "z.backward()\n",
    "x_grad = x.grad\n",
    "print(x_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27c6bc",
   "metadata": {},
   "source": [
    "## 利用autograd.grad方法求导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21780b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# f(x) = a*x^2 + b*x + c \n",
    "x = torch.tensor(0.0, requires_grad = True)\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "\n",
    "y = a*torch.pow(x,2) + b*x + c\n",
    "\n",
    "# create_graph 设置为True将允许创建更高阶的导数,如求二阶导数\n",
    "dy_dx = torch.autograd.grad(y,x, create_graph=True)[0]\n",
    "print(dy_dx.data)\n",
    "\n",
    "# 求二阶倒数\n",
    "dy2_dx2 = torch.autograd.grad(dy_dx, x)[0]\n",
    "print(dy2_dx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "123d1c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.) tensor(1.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(3.) tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "x1 = torch.tensor(1.0, requires_grad = True)\n",
    "x2 = torch.tensor(2.0, requires_grad = True)\n",
    "\n",
    "y1 = x1*x2\n",
    "y2 = x1+x2\n",
    "\n",
    "# 允许同时对多个自动变量求导数\n",
    "(dy1_dx1, dy1_dx2) = torch.autograd.grad(outputs=y1, inputs=[x1,x2], retain_graph=True)\n",
    "print(dy1_dx1, dy1_dx2)\n",
    "(dy2_dx1, dy2_dx2) = torch.autograd.grad(outputs=y2, inputs=[x1,x2], retain_graph=True)\n",
    "print(dy2_dx1, dy2_dx2)\n",
    "\n",
    "# 如果有多个因变量，相当于把多个因变量的梯度结果求和\n",
    "(dy12_dx1, dy12_dx2) = torch.autograd.grad(outputs=[y1, y2],inputs=[x1, x2])\n",
    "print(dy12_dx1, dy12_dx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b255c97",
   "metadata": {},
   "source": [
    "## 利用自动微分和优化器求最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7d081fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y =  tensor(0.)\n",
      "x =  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "# f(x) = a*x^2 + b*x + c \n",
    "x = torch.tensor(0.0, requires_grad = True)\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.SGD(params=[x], lr = 0.01)\n",
    "\n",
    "def f(x):\n",
    "    result = a*torch.pow(x,2)+b*x+c\n",
    "    return(result)\n",
    "\n",
    "for i in range(500):\n",
    "    # 梯度清零\n",
    "    optimizer.zero_grad()\n",
    "    y = f(x)\n",
    "    # 反向传播求梯度\n",
    "    y.backward()\n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "    \n",
    "print(\"y = \", f(x).data)\n",
    "print(\"x = \", x.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b188b",
   "metadata": {},
   "source": [
    "# 动态计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79953017",
   "metadata": {},
   "source": [
    "    本节Pytorch的动态计算图：\n",
    "    \n",
    "    1. 动态计算图简介\n",
    "    2. 计算图中的Function\n",
    "    3. 计算图和反向传播\n",
    "    4. 叶子节点和非叶子节点\n",
    "    5. 计算图在TensorBoard中的可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a298f349",
   "metadata": {},
   "source": [
    "## 动态计算图简介\n",
    "\n",
    "计算图是用来描述运算的有向无环图，计算图有两个主要元素：**结点（Node）和边（Edge）**\n",
    "\n",
    "说法一：Pytorch的计算图由节点和边组成，节点表示张量或者Function，边表示张量和Function之间的依赖关系\n",
    "\n",
    "说法二：\n",
    "- 结点表示数据，如向量，矩阵，张量, function \n",
    "- 边表示运算，如加减乘除卷积等\n",
    "\n",
    "我的想法：节点表示的是数据和函数，边表示的是依赖关系\n",
    "Pytorch中的计算图是动态图。这里的动态主要有两重含义:\n",
    "\n",
    "1. **计算图的正向传播是立即执行的**。无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到计算结果。\n",
    "2. **计算图在反向传播后立即销毁**。下次调用需要重新构建计算图。如果在程序中使用了backward方法执行了反向传播，或者利用torch.autograd.grad方法计算了梯度，那么创建的计算图会被立即销毁，释放存储空间，下次调用需要重新创建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894667f",
   "metadata": {},
   "source": [
    "**一、计算图的正向传播是立即执行的**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "670a1e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.6364)\n",
      "tensor([[ 2.9136],\n",
      "        [ 5.3460],\n",
      "        [ 8.8638],\n",
      "        [ 6.9459],\n",
      "        [ 2.8933],\n",
      "        [ 6.8978],\n",
      "        [-4.3608],\n",
      "        [ 7.4888],\n",
      "        [ 0.7668],\n",
      "        [ 5.7776]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor([[3.0,1.0]],requires_grad = True)\n",
    "b = torch.tensor([[3.0]], requires_grad = True)\n",
    "\n",
    "# 生成一个10*2的矩阵 \n",
    "X = torch.randn(10,2)\n",
    "# 生成一个10*1的矩阵\n",
    "Y = torch.randn(10,1)\n",
    "# Y_hat定义后其正向传播被立即执行，与后面的loss创建语句无关\n",
    "\n",
    "# @是Python 3.5之后加入的矩阵乘法运算符\n",
    "Y_hat = X@w.t() + b \n",
    "# 均方差\n",
    "loss = torch.mean(torch.pow(Y_hat-Y,2))\n",
    "\n",
    "print(loss.data)\n",
    "print(Y_hat.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2bb466",
   "metadata": {},
   "source": [
    "**二、计算图在反向传播后立即销毁**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77478e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "\n",
    "w = torch.tensor([[3.0,1.0]],requires_grad = True)\n",
    "b = torch.tensor([[3.0]], requires_grad = True)\n",
    "\n",
    "# 生成一个10*2的矩阵 \n",
    "X = torch.randn(10,2)\n",
    "# 生成一个10*1的矩阵\n",
    "Y = torch.randn(10,1)\n",
    "# Y_hat定义后其正向传播被立即执行，与后面的loss创建语句无关\n",
    "\n",
    "# @是Python 3.5之后加入的矩阵乘法运算符\n",
    "Y_hat = X@w.t() + b \n",
    "# 均方差\n",
    "loss = torch.mean(torch.pow(Y_hat-Y,2))\n",
    "\n",
    "# 计算图在反向传播完成后立即被销毁，如果需要保留计算图，则需要设置retain_graph = True 的参数\n",
    "# loss.backward(retain_graph=True)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351da5d",
   "metadata": {},
   "source": [
    "## 计算图中的Function \n",
    "\n",
    "计算图中的 张量我们已经比较熟悉了, 计算图中的另外一种节点是Function, 实际上就是 Pytorch中各种对张量操作的函数。\n",
    "\n",
    "这些Function和我们Python中的函数有一个较大的区别，那就是它同时包括正向计算逻辑和反向传播的逻辑\n",
    "\n",
    "**我们可以通过继承torch.autograd.Function来创建这种支持反向传播的Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf94fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
